{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP72YVIHbMOjA1yJbapD4me",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-experiments/haystack_2x_dynamic_prompt_builder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Im5xD2ZUjoBb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c8fec8-d294-4561-dc8d-1094bddfae71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: git+https://github.com/deepset-ai/haystack.git@dynamic_prompt_builder#egg=farm-haystack[preview] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.7/48.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for farm-haystack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/deepset-ai/haystack.git@dynamic_prompt_builder#egg=farm-haystack[preview]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "from typing import List\n",
        "\n",
        "from haystack.preview.components.builders import DynamicPromptBuilder\n",
        "from haystack.preview.components.generators import GPTGenerator\n",
        "from haystack.preview.components.generators.chat import GPTChatGenerator\n",
        "from haystack.preview.dataclasses import ChatMessage, Document\n",
        "from haystack.preview import component, Pipeline\n",
        "\n",
        "\n",
        "logging.disable(logging.CRITICAL)"
      ],
      "metadata": {
        "id": "F-ZMk5Trj05L"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enter OpenAI API key"
      ],
      "metadata": {
        "id": "QcDksfqtlufy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_api_key = getpass.getpass(\"Enter LLM provider api key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWokX06TkDM0",
        "outputId": "9ad3e8bb-0660-4668-c0ff-ca8c74a3b493"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LLM provider api key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use case 1 - ChatMessage(s), no pipeline variables"
      ],
      "metadata": {
        "id": "wg9s5rNtleaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# no parameter init, we don't use any runtime template variables\n",
        "prompt_builder = DynamicPromptBuilder()\n",
        "llm = GPTChatGenerator(api_key=llm_api_key, model_name=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "VUY3cJlqkMJD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline()\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", llm)\n",
        "pipe.connect(\"prompt_builder.prompt\", \"llm.messages\")"
      ],
      "metadata": {
        "id": "ch-5lPbnkR-o"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "location = \"Berlin\"\n",
        "messages = [ChatMessage.from_system(\"Always respond in German even if some input data is in other languages. Be brief.\"),\n",
        "            ChatMessage.from_user(\"Tell me about {{location}}\")]\n",
        "\n",
        "pipe.run(data={\"prompt_builder\": {\"template_variables\":{\"location\": location}, \"prompt_source\": messages}})"
      ],
      "metadata": {
        "id": "2yTB38mCkVMn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9893ea7-ef3e-4da0-f497-10a2a91f9b1f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llm': {'replies': [ChatMessage(content='Berlin ist die Hauptstadt Deutschlands und eine der wichtigsten Städte Europas. Sie hat eine reiche Geschichte und ist für ihre lebendige Kultur, ihre multikulturelle Vielfalt und ihre pulsierende Nachtleben bekannt. Berlin bietet eine Vielzahl von Sehenswürdigkeiten und Attraktionen, darunter das Brandenburger Tor, den Berliner Dom, den Checkpoint Charlie und den Alexanderplatz. Die Stadt hat auch eine blühende Kunst- und Musikszene sowie eine Vielzahl von Museen, Galerien und Theatern. Berlin ist außerdem ein wichtiges wirtschaftliches und politisches Zentrum und beheimatet zahlreiche Unternehmen, Institutionen und Botschaften.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, metadata={'model': 'gpt-3.5-turbo-0613', 'index': 0, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 32, 'completion_tokens': 158, 'total_tokens': 190}})]}}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use case 2 - ChatMessage(s), pipeline variables\n",
        "#### We'll use the pipeline that has both runtime inputs (i.e. documents) as well as user provided template variables (i.e. query)"
      ],
      "metadata": {
        "id": "uh-jz_uHQSX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we'll use documents runtime variables in our template, so we'll let DynamicPromptBuilder know\n",
        "prompt_builder = DynamicPromptBuilder(expected_runtime_variables=[\"documents\"])\n",
        "llm = GPTChatGenerator(api_key=llm_api_key, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "@component\n",
        "class DocumentProducer:\n",
        "\n",
        "  @component.output_types(documents=List[Document])\n",
        "  def run(self, doc_input: str):\n",
        "    return {\"documents\": [Document(content=doc_input)]}\n",
        "\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"doc_producer\", DocumentProducer())\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", llm)\n",
        "pipe.connect(\"doc_producer.documents\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder.prompt\", \"llm.messages\")"
      ],
      "metadata": {
        "id": "liC348vCOz6N"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [ChatMessage.from_system(\"Be helpful assistant, but brief!\"),\n",
        "            ChatMessage.from_user(\"Here is the document: {{documents[0].content}} Now, answer the following: {{query}}\")]\n",
        "\n",
        "pipe.run(data={\"doc_producer\": {\"doc_input\": \"Hello world, I'm Haystack!\"},\n",
        "               \"prompt_builder\": {\"prompt_source\": messages, \"template_variables\":{\"query\": \"who's making a greeting?\"}}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afS3X1m4Qqls",
        "outputId": "339d7f11-fbd6-4427-8fb3-adb899f9bd55"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llm': {'replies': [ChatMessage(content='The greeting is being made by Haystack.', role=<ChatRole.ASSISTANT: 'assistant'>, name=None, metadata={'model': 'gpt-3.5-turbo-0613', 'index': 0, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 43, 'completion_tokens': 9, 'total_tokens': 52}})]}}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use case 3 - non-chat messages\n",
        "#### We'll use dynamic templates for non-chat messages"
      ],
      "metadata": {
        "id": "Ggyxyqlank4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_builder = DynamicPromptBuilder(expected_runtime_variables=[\"documents\"], chat_mode=False)\n",
        "llm = GPTGenerator(api_key=llm_api_key, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "\n",
        "@component\n",
        "class DocumentProducer:\n",
        "\n",
        "  @component.output_types(documents=List[Document])\n",
        "  def run(self, doc_input: str):\n",
        "    return {\"documents\": [Document(content=doc_input)]}\n",
        "\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"doc_producer\", DocumentProducer())\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", llm)\n",
        "pipe.connect(\"doc_producer.documents\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder.prompt\", \"llm.prompt\")"
      ],
      "metadata": {
        "id": "EEAy2bVLh2kn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipeline is setup, but we change template for each pipeline invocation"
      ],
      "metadata": {
        "id": "lHXi8LqBo2AL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Here is the document: {{documents[0].content}} Now, answer the following: {{query}}\"\n",
        "pipe.run(data={\"doc_producer\": {\"doc_input\": \"Hello world, my name is Vladimir\"},\n",
        "               \"prompt_builder\": {\"prompt_source\": template, \"template_variables\":{\"query\": \"who's making a greeting?\"}}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C26xb4VXoNLs",
        "outputId": "e6f6f76e-2a86-403d-8db5-10ac53ea2d22"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llm': {'replies': ['The greeting is being made by the person who wrote the document, which is Vladimir.'],\n",
              "  'metadata': [{'model': 'gpt-3.5-turbo-0613',\n",
              "    'index': 0,\n",
              "    'finish_reason': 'stop',\n",
              "    'usage': {'prompt_tokens': 31,\n",
              "     'completion_tokens': 17,\n",
              "     'total_tokens': 48}}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"Here is the document: {{documents[0].content}} \\n Answer: {{query}}\"\n",
        "pipe.run(data={\"doc_producer\": {\"doc_input\": \"Hello world, I live in Berlin\"},\n",
        "               \"prompt_builder\": {\"prompt_source\": template, \"template_variables\":{\"query\": \"Where does the speaker live?\"}}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGGxGEfXogIC",
        "outputId": "e5cd4356-e402-4e28-89d9-12379a850a8e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'llm': {'replies': ['The speaker lives in Berlin.'],\n",
              "  'metadata': [{'model': 'gpt-3.5-turbo-0613',\n",
              "    'index': 0,\n",
              "    'finish_reason': 'stop',\n",
              "    'usage': {'prompt_tokens': 28,\n",
              "     'completion_tokens': 6,\n",
              "     'total_tokens': 34}}]}}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}