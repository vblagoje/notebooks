{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNFHSBv5dpX48OPlkV15utV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-experiments/chat_hf_local_summarize_graham.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y haystack-ai llmx transformers"
      ],
      "metadata": {
        "id": "3CIYnI26hKeE",
        "outputId": "702bdac0-3daf-4ea7-c757-5103ac8ce47a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: haystack-ai 2.0.0b4\n",
            "Uninstalling haystack-ai-2.0.0b4:\n",
            "  Successfully uninstalled haystack-ai-2.0.0b4\n",
            "\u001b[33mWARNING: Skipping llmx as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: transformers 4.37.0.dev0\n",
            "Uninstalling transformers-4.37.0.dev0:\n",
            "  Successfully uninstalled transformers-4.37.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q autoawq accelerate git+https://github.com/huggingface/transformers.git git+https://github.com/deepset-ai/haystack.git@hf_chat_support"
      ],
      "metadata": {
        "id": "NQ6qMjpsSbBc",
        "outputId": "1ddbf46c-4d6b-4aa5-c62d-461e71eaaea3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for haystack-ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from haystack.components.generators.utils import default_streaming_callback\n",
        "from haystack.components.builders import DynamicChatPromptBuilder\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.converters import HTMLToDocument\n",
        "from haystack.components.generators.chat import HuggingFaceLocalChatGenerator\n",
        "from haystack.dataclasses import ChatMessage\n",
        "from haystack import Pipeline"
      ],
      "metadata": {
        "id": "qZE_yF23RC9B"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lcf = LinkContentFetcher(user_agents=[\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36\"])\n",
        "html_converter = HTMLToDocument(extractor_type=\"ArticleExtractor\")\n",
        "\n",
        "prompt_builder = DynamicChatPromptBuilder(runtime_variables=[\"documents\"])"
      ],
      "metadata": {
        "id": "UzV94CbomLJU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceLocalChatGenerator(model=\"TheBloke/OpenHermes-2.5-Mistral-7B-16k-AWQ\",\n",
        "                                    huggingface_pipeline_kwargs={\"device_map\": \"auto\"},\n",
        "                                    streaming_callback=default_streaming_callback)"
      ],
      "metadata": {
        "id": "kLzMw-zKRFvA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline()\n",
        "pipe.add_component(\"fetcher\", lcf)\n",
        "pipe.add_component(\"converter\", html_converter)\n",
        "pipe.add_component(\"prompt_builder\", prompt_builder)\n",
        "pipe.add_component(\"llm\", llm)\n",
        "\n",
        "\n",
        "pipe.connect(\"fetcher.streams\", \"converter.sources\")\n",
        "pipe.connect(\"converter.documents\", \"prompt_builder.documents\")\n",
        "pipe.connect(\"prompt_builder.prompt\", \"llm.messages\")"
      ],
      "metadata": {
        "id": "UtsgW1Adm8_M"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template_prefix = \"\"\"Given the article below: \\n\n",
        "            {% for document in documents %}\n",
        "                {{ document.content }}\n",
        "            {% endfor %}\n",
        "            {{prompt_suffix}}\n",
        "            \"\"\"\n",
        "\n",
        "messages = [ChatMessage.from_user(template_prefix)]\n"
      ],
      "metadata": {
        "id": "D1GsXe82pw8w"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe.run(data={\"urls\": [\"https://www.paulgraham.com/superlinear.html\"],\n",
        "                        \"prompt_source\": messages,\n",
        "                        \"template_variables\": {\"prompt_suffix\" : \"Summarize the main takeaways and learnings\"},\n",
        "                        \"generation_kwargs\": {\"prompt_lookup_num_tokens\": 10}})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQT2zU9AibSS",
        "outputId": "f9bc8fae-a969-4efe-b9e1-ad1666bc9bdd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The article discusses the concept of superlinear returns, which refers to the idea that the returns for performance are not proportional to the input, but rather grow exponentially. The author explains that this phenomenon is prevalent in various aspects of life, such as business, fame, power, military victories, knowledge, and even the benefit to humanity. The two main causes of superlinear returns are exponential growth and thresholds. Exponential growth leads to thresholds, which in turn lead to superlinear returns. The author suggests that ambitious individuals should seek work that compounds, meaning work that grows exponentially or teaches you something, and emphasizes the importance of curiosity, interest, and independence in finding fields with superlinear returns. The article also highlights the increasing variation in outcomes due to the shift away from institutions and the importance of learning from failure. The author provides a recipe for achieving superlinear returns, which includes choosing work you have a natural aptitude for and a deep interest in, working hard without burning out, seeking out the best colleagues, and following your curiosity. The article also mentions that the most extreme returns come from expanding the territory of superlinear returns and that the most ambitious and independent-minded individuals will likely benefit the most from this phenomenon.<|im_end|>\n"
          ]
        }
      ]
    }
  ]
}