{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4/0WknrYkaaBmXldDikG4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-demos/haystack_anthropic_rag_firecrawl_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook provides a detailed guide on leveraging the upcoming Haystack 2.3 and the Firecrawl scrape service for implementing an OpenAPI service-based Retriever-Augmented Generation (RAG) workflow. It explains how to process user queries by scraping web pages and using the retrieved text data to answer questions, a classic RAG on the web use case.\n",
        "\n",
        "The pipeline starts by fetching the OpenAPI specification for Firecrawl and converting it into OpenAI function-calling definitions. Firecrawl scrapes relevant information from the web based on user input, providing structured markdown for easy integration with LLMs.\n",
        "\n",
        "You'll be guided through installing necessary libraries, selecting an LLM provider, and constructing a Haystack 2.0 pipeline that orchestrates function calling, service requests, and LLM response generation.\n",
        "\n",
        "Note: A Firecrawl account is required to run this pipeline. Signing up is straightforward and provides access to their scraping capabilities.\n",
        "\n",
        "For more details on Firecrawl and its features, refer to the [Firecrawl documentation](https://docs.firecrawl.dev/introduction)."
      ],
      "metadata": {
        "id": "J8TDTtQwiSn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "Let's install necessary libraries and import key modules to build the foundation for the subsequent steps."
      ],
      "metadata": {
        "id": "6zUWNgF-kUeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q anthropic-haystack git+https://github.com/deepset-ai/haystack-experimental.git@openapi"
      ],
      "metadata": {
        "id": "bH8Lo7PSumLG",
        "outputId": "976b7e65-254e-4f13-cbe3-2282da82d663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m862.7/862.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m327.6/327.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for haystack-experimental (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.components.converters import OutputAdapter\n",
        "from haystack.dataclasses import ChatMessage, ByteStream\n",
        "from haystack.utils import Secret\n",
        "\n",
        "from haystack_integrations.components.generators.anthropic import AnthropicChatGenerator\n",
        "from haystack_experimental.components.tools.openapi import OpenAPITool, LLMProvider"
      ],
      "metadata": {
        "id": "RhxdVLCb_D8R"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  Enter the API keys for LLM provider and serper.dev"
      ],
      "metadata": {
        "id": "V8GwXloVDtg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_api_key = getpass.getpass(f\"Enter Anthropic api key for:\")\n",
        "firecrawl_dev_key = getpass.getpass(\"Enter Firecrawl api key:\")"
      ],
      "metadata": {
        "id": "TrK-E_CF__MG",
        "outputId": "0068841e-bbb3-4b81-fba7-98581c38acff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Anthropic api key for:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Enter Firecrawl api key:Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build our RAG Web QA pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "0N4yjfJHD8HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline()\n",
        "pipe.add_component(\"firecrawl\", OpenAPITool(generator_api=LLMProvider.ANTHROPIC, generator_api_params={\"model\":\"claude-3-opus-20240229\",\n",
        "                                                                  \"api_key\":Secret.from_token(llm_api_key)},\n",
        "                                            spec=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/apps/api/openapi.json\",\n",
        "                                            credentials=Secret.from_token(firecrawl_dev_key)))\n",
        "\n",
        "pipe.add_component(\"final_prompt_adapter\", OutputAdapter(\"{{system_message + service_response}}\", List[ChatMessage]))\n",
        "pipe.add_component(\"llm\", AnthropicChatGenerator(api_key=Secret.from_token(llm_api_key), model=\"claude-3-opus-20240229\",\n",
        "                                              generation_kwargs={\"max_tokens\": 1024},\n",
        "                                              streaming_callback=print_streaming_chunk))\n",
        "\n",
        "\n",
        "pipe.connect(\"firecrawl\", \"final_prompt_adapter.service_response\")\n",
        "pipe.connect(\"final_prompt_adapter\", \"llm.messages\")"
      ],
      "metadata": {
        "id": "n7ErNg55_zJN",
        "outputId": "8a363da9-5b28-402c-fec1-3e33c4b60b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x7a466e295ed0>\n",
              "ğŸš… Components\n",
              "  - firecrawl: OpenAPITool\n",
              "  - final_prompt_adapter: OutputAdapter\n",
              "  - llm: AnthropicChatGenerator\n",
              "ğŸ›¤ï¸ Connections\n",
              "  - firecrawl.service_response -> final_prompt_adapter.service_response (List[ChatMessage])\n",
              "  - final_prompt_adapter.output -> llm.messages (List[ChatMessage])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the pipeline graph above, for a given firecrawl request, our pipeline follows these steps:\n",
        "\n",
        "1. **Fetch OpenAPI Spec**: Retrieve the OpenAPI specification for firecrawl and convert it into OpenAI function-calling definitions.\n",
        "\n",
        "2. **Determine Parameters**: Use a function-calling model to identify the necessary parameters for the firecrawl service.\n",
        "\n",
        "3. **Invoke firecrawl**: Dispatch the request to firecrawl with the determined parameters and gather the responses.\n",
        "\n",
        "4. **Compile Results**: Organize and format the search results from firecrawl.\n",
        "\n",
        "5. **Generate Response**: Combine the system prompt, user query, and compiled search results, then pass them to LLM to generate the final response."
      ],
      "metadata": {
        "id": "1rawAmKSELBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Given the article below, answer: What's the main takeaway from this article? Be insightful and elaborate, base your answer on the article only.\""
      ],
      "metadata": {
        "id": "msBEddD4Azfh"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe.run(data={\"firecrawl\": {\"messages\": [ChatMessage.from_user(\"Scrape https://haystack.deepset.ai/blog/rag-evaluation-with-prometheus-2\")]},\n",
        "                        \"final_prompt_adapter\": {\"system_message\": [ChatMessage.from_system(user_prompt)]}})\n"
      ],
      "metadata": {
        "id": "OVcVXOSRAfrm",
        "outputId": "993f097c-982b-48ba-8a28-f78d92ec1110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main takeaway from this article is that Prometheus 2, a newly released family of open-source models specifically trained for evaluation, can effectively assess the quality of responses produced by retrieval-augmented generation (RAG) pipelines in Haystack along several axes.\n",
            "\n",
            "The article first discusses the importance of evaluation when building real-world applications based on language models like RAG. It notes the limitations of using proprietary models for evaluation, such as data privacy concerns, lack of transparency, and high costs. \n",
            "\n",
            "It then introduces Prometheus 2, which bridges the gap between proprietary and open models for evaluation. Prometheus 2 models unify direct assessment and pairwise ranking paradigms, correlate well with human judgment, and allow customizable evaluation criteria.\n",
            "\n",
            "The article provides examples of how to prompt Prometheus 2 models effectively to evaluate generated answers. It then demonstrates integrating Prometheus 2 into Haystack by creating a custom PrometheusLLMEvaluator component. This component enables building different evaluators to assess aspects like correctness, relevance, and logical robustness of RAG pipeline outputs.\n",
            "\n",
            "Experiments with a small RAG pipeline in Haystack showcase promising results using Prometheus 2 for multifaceted evaluation. The article concludes that while further assessment for specific use cases is prudent, Prometheus 2 models offer an interesting and promising open-source solution for comprehensively evaluating language model applications today. It suggests general-purpose open models may soon provide effective evaluation capabilities as well."
          ]
        }
      ]
    }
  ]
}