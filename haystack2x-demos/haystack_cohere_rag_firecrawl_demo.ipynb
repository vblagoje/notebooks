{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP3fgroiqYk4ZtYyC5LDXuH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-demos/haystack_rag_firecrawl_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook provides a detailed guide on leveraging the upcoming Haystack 2.3 and the Firecrawl scrape service for implementing an OpenAPI service-based Retriever-Augmented Generation (RAG) workflow. It explains how to process user queries by scraping web pages and using the retrieved text data to answer questions, a classic RAG on the web use case.\n",
        "\n",
        "The pipeline starts by fetching the OpenAPI specification for Firecrawl and converting it into OpenAI function-calling definitions. Firecrawl scrapes relevant information from the web based on user input, providing structured markdown for easy integration with LLMs.\n",
        "\n",
        "You'll be guided through installing necessary libraries, selecting an LLM provider, and constructing a Haystack 2.0 pipeline that orchestrates function calling, service requests, and LLM response generation.\n",
        "\n",
        "Note: A Firecrawl account is required to run this pipeline. Signing up is straightforward and provides access to their scraping capabilities.\n",
        "\n",
        "For more details on Firecrawl and its features, refer to the [Firecrawl documentation](https://docs.firecrawl.dev/introduction)."
      ],
      "metadata": {
        "id": "J8TDTtQwiSn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "Let's install necessary libraries and import key modules to build the foundation for the subsequent steps."
      ],
      "metadata": {
        "id": "6zUWNgF-kUeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q cohere-haystack git+https://github.com/deepset-ai/haystack-experimental.git@openapi"
      ],
      "metadata": {
        "id": "bH8Lo7PSumLG",
        "outputId": "2ae5b85f-d1df-4842-92f8-4deb558b8cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.8/173.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for haystack-experimental (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.components.converters import OutputAdapter\n",
        "from haystack.dataclasses import ChatMessage, ByteStream\n",
        "from haystack.utils import Secret\n",
        "\n",
        "from haystack_integrations.components.generators.cohere import CohereChatGenerator\n",
        "from haystack_experimental.components.tools.openapi import OpenAPITool, LLMProvider"
      ],
      "metadata": {
        "id": "RhxdVLCb_D8R"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.  Enter the API keys for LLM provider and serper.dev"
      ],
      "metadata": {
        "id": "V8GwXloVDtg9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_api_key = getpass.getpass(f\"Enter Cohere api key for:\")\n",
        "firecrawl_dev_key = getpass.getpass(\"Enter Firecrawl api key:\")"
      ],
      "metadata": {
        "id": "TrK-E_CF__MG",
        "outputId": "1c99dcfb-3894-4696-a4bb-43a6f06c51a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Cohere api key for:··········\n",
            "Enter Firecrawl api key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Build our RAG Web QA pipeline\n",
        "\n"
      ],
      "metadata": {
        "id": "0N4yjfJHD8HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline()\n",
        "pipe.add_component(\"firecrawl\", OpenAPITool(generator_api=LLMProvider.COHERE,\n",
        "                                            generator_api_params={\"model\":\"command-r\",\n",
        "                                                                  \"api_key\":Secret.from_token(llm_api_key)},\n",
        "                                            spec=\"https://raw.githubusercontent.com/mendableai/firecrawl/main/apps/api/openapi.json\",\n",
        "                                            credentials=Secret.from_token(firecrawl_dev_key)))\n",
        "\n",
        "pipe.add_component(\"final_prompt_adapter\", OutputAdapter(\"{{system_message + service_response}}\", List[ChatMessage]))\n",
        "pipe.add_component(\"llm\", CohereChatGenerator(api_key=Secret.from_token(llm_api_key), model=\"command-r\",\n",
        "                                              generation_kwargs={\"max_tokens\": 1024},\n",
        "                                              streaming_callback=print_streaming_chunk))\n",
        "\n",
        "\n",
        "pipe.connect(\"firecrawl\", \"final_prompt_adapter.service_response\")\n",
        "pipe.connect(\"final_prompt_adapter\", \"llm.messages\")"
      ],
      "metadata": {
        "id": "n7ErNg55_zJN",
        "outputId": "b845d10e-b21d-47ff-d224-1db7d661abfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<haystack.core.pipeline.pipeline.Pipeline object at 0x79fdbec05b70>\n",
              "🚅 Components\n",
              "  - firecrawl: OpenAPITool\n",
              "  - final_prompt_adapter: OutputAdapter\n",
              "  - llm: CohereChatGenerator\n",
              "🛤️ Connections\n",
              "  - firecrawl.service_response -> final_prompt_adapter.service_response (List[ChatMessage])\n",
              "  - final_prompt_adapter.output -> llm.messages (List[ChatMessage])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see in the pipeline graph above, for a given firecrawl request, our pipeline follows these steps:\n",
        "\n",
        "1. **Fetch OpenAPI Spec**: Retrieve the OpenAPI specification for firecrawl and convert it into OpenAI function-calling definitions.\n",
        "\n",
        "2. **Determine Parameters**: Use a function-calling model to identify the necessary parameters for the firecrawl service.\n",
        "\n",
        "3. **Invoke firecrawl**: Dispatch the request to firecrawl with the determined parameters and gather the responses.\n",
        "\n",
        "4. **Compile Results**: Organize and format the search results from firecrawl.\n",
        "\n",
        "5. **Generate Response**: Combine the system prompt, user query, and compiled search results, then pass them to LLM to generate the final response."
      ],
      "metadata": {
        "id": "1rawAmKSELBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Given the article below, answer: What's the main takeaway from this article? Be insightful and elaborate, base your answer on the article only.\""
      ],
      "metadata": {
        "id": "msBEddD4Azfh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipe.run(data={\"firecrawl\": {\"messages\": [ChatMessage.from_user(\"Scrape https://haystack.deepset.ai/blog/rag-evaluation-with-prometheus-2\")]},\n",
        "                        \"final_prompt_adapter\": {\"system_message\": [ChatMessage.from_user(user_prompt)]}})\n"
      ],
      "metadata": {
        "id": "OVcVXOSRAfrm",
        "outputId": "2ffc7531-610f-483f-bf6a-33de82e3a200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The main takeaway from the article is the introduction of Prometheus 2, a cutting-edge open-source model specifically designed for evaluating the output of language models. What sets Prometheus 2 apart is its ability to bridge the gap between proprietary models and open LMs for evaluation purposes. The model is trained to perform direct assessments and pairwise rankings, which makes it a versatile tool for evaluating language models.\n",
            "\n",
            "The article also serves as a comprehensive guide on how to leverage Prometheus 2 within the Haystack framework. Haystack users can now create custom evaluators using Prometheus 2, enabling them to assess their models' performance along various dimensions, such as correctness, response relevance, and logical robustness. This enhancement empowers developers to fine-tune their language models and RAG pipelines, ensuring they deliver accurate and meaningful responses.\n",
            "\n",
            "By employing Prometheus 2 as an evaluator, users can benefit from a robust evaluation process without compromising data privacy, transparency, or control over the model's behavior. Moreover, the model's high correlation with human evaluations and proprietary language models underscores its reliability. The article's author, Stefano Fiorucci, an open-source engineer, encourages readers to experiment with Prometheus 2 in Haystack, providing a step-by-step guide and practical examples.\n",
            "\n",
            "In conclusion, Prometheus 2 represents a significant advancement in open-source model evaluation, offering an effective approach to assess the output of language models. Its integration into Haystack opens up new possibilities for developers to build and refine their LLM-based applications with confidence."
          ]
        }
      ]
    }
  ]
}