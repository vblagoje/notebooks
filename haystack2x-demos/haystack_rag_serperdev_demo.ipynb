{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOK6F90zvJOqlDXC/LmwxXZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-demos/haystack_rag_serperdev_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook showcases the upcoming Haystack OpenAPI service-based Retriever-Augmented Generation (RAG). Given a user query, we search the web for the results of the query and inject these results into LLM context along with the system prompt."
      ],
      "metadata": {
        "id": "J8TDTtQwiSn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "\n",
        "Let's install necessary libraries and import key modules to build the foundation for the subsequent steps."
      ],
      "metadata": {
        "id": "6zUWNgF-kUeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y llmx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTo7qxZa_02g",
        "outputId": "d4511548-65e2-4cd6-9a18-6face7572dca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: llmx 0.0.15a0\n",
            "Uninstalling llmx-0.0.15a0:\n",
            "  Successfully uninstalled llmx-0.0.15a0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openapi3 jsonref"
      ],
      "metadata": {
        "id": "bH8Lo7PSumLG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/deepset-ai/haystack.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkB9-FamGO7C",
        "outputId": "7664d43e-5d0f-4132-fc19-81c9265650bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for haystack-ai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "\n",
        "from haystack import Pipeline\n",
        "from haystack.components.generators.utils import default_streaming_callback\n",
        "from haystack.components.converters import OpenAPIServiceToFunctions\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.connectors import OpenAPIServiceConnector\n",
        "from haystack.dataclasses import ChatMessage"
      ],
      "metadata": {
        "id": "RhxdVLCb_D8R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare SerperDev service along with system prompt\n",
        "\n",
        "- Generate OpenAI functions definitions for SerperDev"
      ],
      "metadata": {
        "id": "m-Po2hm0kzKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_func_pipeline = Pipeline()\n",
        "gen_func_pipeline.add_component(\"spec_to_functions\", OpenAPIServiceToFunctions())"
      ],
      "metadata": {
        "id": "rXrLxTQ8Dl8V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "functions_result = gen_func_pipeline.run(data={\"sources\":[\"https://bit.ly/3NIJqnd\"],\n",
        "                                               \"system_messages\":[requests.get(\"https://bit.ly/3TdHsyB\").text]})"
      ],
      "metadata": {
        "id": "F7rKAns3Do6Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. API keys, set up simple authentication mechanism"
      ],
      "metadata": {
        "id": "JTZRxFD5lX7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_api_key = getpass.getpass(\"Enter LLM provider api key:\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNGXm479JSr4",
        "outputId": "85b05bd2-1aa0-4e5c-fb1e-bf9d6d26f3aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LLM provider api key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "serper_dev_key = getpass.getpass(\"Enter serperdev api key:\")\n",
        "services_auth = {\"SerperDev\":serper_dev_key}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqGsQrRYJZ-2",
        "outputId": "455beadf-db8e-43d7-d393-8914afa5b3af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter serperdev api key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Retrieval step - SerperDev service invocation"
      ],
      "metadata": {
        "id": "X4uOPWS-lq2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "invoke_service_pipe = Pipeline()\n",
        "invoke_service_pipe.add_component(\"functions_llm\", OpenAIChatGenerator(api_key=llm_api_key, model_name=\"gpt-3.5-turbo-0613\"))\n",
        "invoke_service_pipe.add_component(\"openapi_container\", OpenAPIServiceConnector(services_auth))\n",
        "invoke_service_pipe.connect(\"functions_llm.replies\", \"openapi_container.messages\")"
      ],
      "metadata": {
        "id": "L7MZib9gJ4j-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_prompt = \"Why was Sam Altman ousted from OpenAI?\""
      ],
      "metadata": {
        "id": "AF25DrntJ1mP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "service_desc_document = functions_result[\"spec_to_functions\"][\"documents\"][0]\n",
        "openai_functions_definition = json.loads(service_desc_document.content)\n",
        "openapi_spec = service_desc_document.meta[\"spec\"]\n",
        "\n",
        "service_response = invoke_service_pipe.run(data={\"messages\":[ChatMessage.from_user(user_prompt)],\n",
        "                                                 \"generation_kwargs\": {\"functions\": [openai_functions_definition]},\n",
        "                                                 \"service_openapi_spec\": openapi_spec})"
      ],
      "metadata": {
        "id": "OHP2Nj79KCdQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generate LLM response\n",
        "\n",
        "Inject service response into LLM context, pair it with system prompt"
      ],
      "metadata": {
        "id": "S9pPCVzwmHpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gen_pipe = Pipeline()\n",
        "llm = OpenAIChatGenerator(api_key=llm_api_key, model_name=\"gpt-4-1106-preview\", streaming_callback=default_streaming_callback)\n",
        "gen_pipe.add_component(\"llm\", llm)\n",
        "\n",
        "github_pr_prompt_messages = [ChatMessage.from_system(service_desc_document.meta[\"system_message\"])] + service_response[\"openapi_container\"][\"service_response\"]\n",
        "final_result = gen_pipe.run(data={\"messages\": github_pr_prompt_messages})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnYwWtYSKFl0",
        "outputId": "776c78fe-3d85-4245-d023-1916398e10b7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the information given, Sam Altman was abruptly fired by OpenAI's board of directors but was brought back to his position as CEO just days later. This executive shuffle stemmed from long-standing tensions within the company. Altman's initial ouster was followed by a mass staff revolt, and after frenzied discussions about the future of the startup, he was rehired. The considerable drama surrounding his departure and rehire has been linked to issues within the company's leadership dynamics and governance."
          ]
        }
      ]
    }
  ]
}