{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNH0kUi5O1qD9oQ5gKf0tcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vblagoje/notebooks/blob/main/haystack2x-demos/haystack_rag_services_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "This notebook showcases a novel application of Qdrant vector DB in the upcoming Haystack OpenAPI service-based Retriever-Augmented Generation (RAG) system. Given a user query, we retrieve the corresponding OpenAPI specification from Qdrant vector DB. The retrieved service specification is then seamlessly injected into the Haystack pipeline, enabling the dynamic invocation of selected API services. This innovative approach, integrating any OpenAPI spec service with LLMs, opens a new era for RAG systems to generate contextually rich responses from any openapi service."
      ],
      "metadata": {
        "id": "J8TDTtQwiSn6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup\n",
        "This notebook demos Haystack 2.x service based RAG, using two openapi services: GitHub's compare_branches and SerperDev search API.\n",
        "\n",
        "Let's install necessary libraries and import key modules to build the foundation for the subsequent steps."
      ],
      "metadata": {
        "id": "6zUWNgF-kUeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"grpcio-tools==1.42\" sentence-transformers openapi3 jsonref qdrant-haystack git+https://github.com/deepset-ai/haystack.git"
      ],
      "metadata": {
        "id": "bH8Lo7PSumLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import json\n",
        "import requests\n",
        "from typing import List, Dict, Any\n",
        "from haystack_integrations.document_stores.qdrant import QdrantDocumentStore\n",
        "from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever\n",
        "\n",
        "from haystack import Pipeline, Document\n",
        "from haystack.components.generators.utils import print_streaming_chunk\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.components.embedders import SentenceTransformersDocumentEmbedder\n",
        "from haystack.components.converters import OpenAPIServiceToFunctions, OutputAdapter\n",
        "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
        "from haystack.components.generators.chat import OpenAIChatGenerator\n",
        "from haystack.components.connectors import OpenAPIServiceConnector\n",
        "from haystack.components.fetchers import LinkContentFetcher\n",
        "from haystack.components.others import Multiplexer\n",
        "from haystack.dataclasses import ChatMessage, ByteStream"
      ],
      "metadata": {
        "id": "RhxdVLCb_D8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Let's collect the API keys for LLM provider, serper.dev, and github token"
      ],
      "metadata": {
        "id": "m3IcPyJnpPMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm_api_key = getpass.getpass(\"Enter LLM provider api key:\")\n",
        "serper_dev_key = getpass.getpass(\"Enter serperdev api key:\")\n",
        "github_token = getpass.getpass(\"Enter your github token:\")"
      ],
      "metadata": {
        "id": "uqGsQrRYJZ-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setup QdrantDocumentStore\n",
        "\n",
        "QdrantDocumentStore is one of many vector DB integrations we offer via Haystack Core Integrations [project](https://github.com/deepset-ai/haystack-core-integrations/)"
      ],
      "metadata": {
        "id": "m-Po2hm0kzKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_store = QdrantDocumentStore(\n",
        "    path=\"./qdrant_101\",\n",
        "    index=\"Document\",\n",
        "    embedding_dim=768,\n",
        "    recreate_index=True\n",
        "  )"
      ],
      "metadata": {
        "id": "mmVOQAg-C9Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Add some helper functions we'll use in both indexing and retrieval pipelines"
      ],
      "metadata": {
        "id": "m31frRXzo-W3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_fc(openai_functions_schema: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    return {\n",
        "        \"tools\": [{\n",
        "            \"type\": \"function\",\n",
        "            \"function\": openai_functions_schema\n",
        "        }],\n",
        "        \"tool_choice\": {\n",
        "            \"type\": \"function\",\n",
        "            \"function\": {\"name\": openai_functions_schema[\"name\"]}\n",
        "        }\n",
        "    }\n",
        "\n",
        "def create_docs(functions, specs, system_messages, service_credentials):\n",
        "  docs = []\n",
        "  for function, spec, system_message, service_credential in zip(functions, specs, system_messages, service_credentials):\n",
        "    d = Document(content=json.dumps(function),\n",
        "                 meta={\"spec\": json.dumps(spec),\n",
        "                       \"system_message\": system_message,\n",
        "                       \"service_credential\": service_credential})\n",
        "    docs.append(d)\n",
        "  return docs"
      ],
      "metadata": {
        "id": "7U-QKL3nyhX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Index two OpenAPI specs: Github (compare_branches) and [SerperDev](https://serper.dev)\n",
        "\n",
        "Each indexed service Document contains the following descriptors:\n",
        "\n",
        "* OpenAI function calling JSON\n",
        "* OpenAPI specification\n",
        "* System message for the service\n",
        "* Service credentials"
      ],
      "metadata": {
        "id": "iz57DpxelJKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(\"fetcher\", LinkContentFetcher())\n",
        "indexing_pipeline.add_component(\"mx\", Multiplexer(List[ByteStream]))\n",
        "indexing_pipeline.add_component(\"spec_to_functions\", OpenAPIServiceToFunctions())\n",
        "indexing_pipeline.add_component(\"embedder\", SentenceTransformersDocumentEmbedder(model=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "                                                                                 meta_fields_to_embed=[\"spec\", \"system_message\", \"service_credential\"]))\n",
        "indexing_pipeline.add_component(\"writer\", DocumentWriter(document_store=document_store))\n",
        "indexing_pipeline.add_component(\"a1\", OutputAdapter(\"{{functions | create_docs(specs, system_messages, service_credentials)}}\",\n",
        "                                                     List[Document],\n",
        "                                                      {\"create_docs\": create_docs}))\n",
        "\n",
        "indexing_pipeline.add_component(\"a2\", OutputAdapter(\"{{sources | json_objects}}\",\n",
        "                                                     List[Dict[str, Any]],\n",
        "                                                      {\"json_objects\": lambda sources: [json.loads(s.to_string()) for s in sources]}))\n",
        "\n",
        "indexing_pipeline.connect(\"fetcher.streams\", \"mx\")\n",
        "indexing_pipeline.connect(\"mx\", \"spec_to_functions.sources\")\n",
        "indexing_pipeline.connect(\"mx\", \"a2.sources\")\n",
        "indexing_pipeline.connect(\"a2\", \"a1.specs\")\n",
        "indexing_pipeline.connect(\"spec_to_functions.functions\", \"a1\")\n",
        "indexing_pipeline.connect(\"a1.output\", \"embedder.documents\")\n",
        "indexing_pipeline.connect(\"embedder\", \"writer\")"
      ],
      "metadata": {
        "id": "rXrLxTQ8Dl8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = indexing_pipeline.run(data={\"fetcher\": {\"urls\":[\"https://bit.ly/github_compare\",\"https://bit.ly/serper_dev_spec\"]},\n",
        "                                     \"a1\":{\"service_credentials\": [github_token, serper_dev_key],\n",
        "                                           \"system_messages\": [requests.get(\"https://bit.ly/pr_auto_system\").text,\n",
        "                                                               requests.get(\"https://bit.ly/serper_dev_system_prompt\").text]\n",
        "                                           }\n",
        "                                     })\n",
        "print(f\"\\nWrote {result['writer']['documents_written']} documents\")"
      ],
      "metadata": {
        "id": "F7rKAns3Do6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Build the retrieval/LLM generation pipeline"
      ],
      "metadata": {
        "id": "JTZRxFD5lX7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from haystack.utils import Secret\n",
        "\n",
        "pipe = Pipeline()\n",
        "pipe.add_component(\"embedder\", SentenceTransformersTextEmbedder(model=\"sentence-transformers/all-mpnet-base-v2\"))\n",
        "pipe.add_component(\"retriever\", QdrantEmbeddingRetriever(document_store=document_store))\n",
        "pipe.add_component(\"mx\", Multiplexer(List[Document]))\n",
        "pipe.add_component(\"a1\", OutputAdapter(\"{{documents[0].content | tojson | prepare_fc}}\", Dict[str, Any], {\"prepare_fc\": prepare_fc, \"tojson\": lambda s: json.loads(s)}))\n",
        "pipe.add_component(\"a2\", OutputAdapter(\"{{documents[0].meta['spec'] | tojson}}\", Dict[str, Any], {\"tojson\": lambda s: json.loads(s)}))\n",
        "pipe.add_component(\"a3\", OutputAdapter(\"{{documents[0].meta['service_credential']}}\", str))\n",
        "pipe.add_component(\"a4\", OutputAdapter(\"{{documents[0].meta['system_message'] | cm}}\", List[ChatMessage], {\"cm\": lambda s: [ChatMessage.from_system(s)]}))\n",
        "pipe.add_component(\"a5\", OutputAdapter(\"{{system_message + service_response}}\", List[ChatMessage], {\"cm\": lambda s: [ChatMessage.from_system(s)]}))\n",
        "pipe.add_component(\"fn_llm\", OpenAIChatGenerator(api_key=Secret.from_token(llm_api_key), model=\"gpt-3.5-turbo-0613\"))\n",
        "pipe.add_component(\"openapi_container\", OpenAPIServiceConnector())\n",
        "pipe.add_component(\"llm\", OpenAIChatGenerator(api_key=Secret.from_token(llm_api_key), model=\"gpt-4-1106-preview\", streaming_callback=print_streaming_chunk))\n",
        "\n",
        "pipe.connect(\"embedder\", \"retriever\")\n",
        "pipe.connect(\"retriever.documents\", \"mx\")\n",
        "pipe.connect(\"mx\", \"a1.documents\")\n",
        "pipe.connect(\"mx\", \"a2.documents\")\n",
        "pipe.connect(\"mx\", \"a3.documents\")\n",
        "pipe.connect(\"mx\", \"a4.documents\")\n",
        "pipe.connect(\"a1\", \"fn_llm.generation_kwargs\")\n",
        "pipe.connect(\"a2\", \"openapi_container.service_openapi_spec\")\n",
        "pipe.connect(\"a3\", \"openapi_container.service_credentials\")\n",
        "pipe.connect(\"a4\", \"a5.system_message\")\n",
        "pipe.connect(\"a5\", \"llm.messages\")\n",
        "pipe.connect(\"openapi_container.service_response\", \"a5.service_response\")\n",
        "pipe.connect(\"fn_llm.replies\", \"openapi_container.messages\")"
      ],
      "metadata": {
        "id": "L7MZib9gJ4j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Retrieval, service invocation and LLM generation\n",
        "\n",
        "Here is where everything gets interesting! Based on the prompt below, we'll retrieve the top_k=1 document from QdrantDocumentStore. That'll be the relevant service Document. We'll then proceed to use service descriptors from the retrieved Document to initiate:\n",
        "\n",
        "* Based on the prompt, we'll use function calling to resolve the parameters for service invocation.\n",
        "* We'll invoke the OpenAPI-compliant service with those parameters.\n",
        "* The response from the service will be joined with the service prompt for that service.\n",
        "* The final LLM response will be generated.\n",
        "\n",
        "\n",
        "This process involves straightforward intent recognition that utilizes vector similarity to accurately identify and retrieve the appropriate service Document. Theoretically, it's possible to have dozens of services indexed within the QdrantDocumentStore. Once we retrieve the right service Documents, we will rely on the Haystack pipeline to perform the magic and observe the response generated by the LLM."
      ],
      "metadata": {
        "id": "ZhpXSiqimXtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select on of these two prompts and run the pipeline\n",
        "user_prompt = \"Search web and tell me why was Sam Altman ousted from OpenAI\"\n",
        "user_prompt = \"Compare branches main and test/benchmarks2.0, in project deepset-ai, repo haystack\"\n",
        "\n",
        "result = pipe.run(data={\"fn_llm\": {\"messages\":[ChatMessage.from_user(user_prompt)]}, \"embedder\": {\"text\": user_prompt}})"
      ],
      "metadata": {
        "id": "OHP2Nj79KCdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Obligatory pretty print version"
      ],
      "metadata": {
        "id": "L2Ico4OYoaST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "display(Markdown(result[\"llm\"][\"replies\"][0].content))\n"
      ],
      "metadata": {
        "id": "H64OuyRwM_PD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thank you, questions?\n",
        "\n",
        "<a href=\"www.qr-code-generator.com/\" border=\"0\" style=\"cursor:default\" rel=\"nofollow\"><img src=\"https://chart.googleapis.com/chart?cht=qr&chl=https%3A%2F%2Fgithub.com%2Fvblagoje%2Fnotebooks%2Fblob%2Fmain%2Fhaystack2x-demos%2Fhaystack_rag_services_demo.ipynb&chs=180x180&choe=UTF-8&chld=L|2\"></a>"
      ],
      "metadata": {
        "id": "PLVhdFF06_p5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Links:\n",
        "- https://github.com/deepset-ai/haystack/\n",
        "- https://haystack.deepset.ai/community\n",
        "- https://haystack.deepset.ai/advent-of-haystack\n",
        "- https://x.com/vladblagoje"
      ],
      "metadata": {
        "id": "sD3JS7ds7IJ2"
      }
    }
  ]
}